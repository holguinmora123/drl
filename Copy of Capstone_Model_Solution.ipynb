{"cells":[{"cell_type":"markdown","metadata":{"id":"TX26m5IA-WaT"},"source":["# Notebook Instructions\n","\n","1. All the <u>code and data files</u> used in this notebook are available in the downloadable unit in the <u>last unit of this section</u>.\n","2. You can run the notebook document sequentially (one cell at a time) by pressing **shift + enter**. \n","3. While a cell is running, a [*] is shown on the left. After the cell is run, the output will appear on the next line.\n","\n","This course is based on specific versions of python packages. You can find the details of the packages in <a href='https://quantra.quantinsti.com/quantra-notebook' target=\"_blank\" >this manual</a>."]},{"cell_type":"markdown","metadata":{"id":"WZst6Ccp-WaY"},"source":["# Capstone Project Model Solution: Building the RL model\n","\n","In this notebook, we provide a model solution to build a reinforcement learning model for the capstone project. You can refer to this if you are stuck in any step while building your solution. The features and parameters added here are not exhaustive or fine-tuned. The RL model needs to be calibrated separately for each underlying asset it trades on.\n","\n","The model solution guides you to answer the problem statements posed in the capstone project. The notebook structure is as follows:\n","\n","1. [Read price data](#Read_price_data): Get any stock data, FX data or any other asset data of your choice. This data needs to be of minute frequency. We have provided a sample FX currency pair data in the model solution. \n","<br>\n","\n","1. [Data sanity check](#sanity): Perform data sanity checks such as the check for missing values and the check for outliers. Then, you can either fix the data or get good quality data.\n","<br>\n","1. [Game class and input features](#input_features): We make use of TA-lib for creating technical features. For fundamental features, we need to obtain data from external sources. Example features added here:\n","    * Statistical features: Beta of high and low values\n","    * Overlap Studies: Change simple moving average to exponential moving average\n","    * Volatility indicator: ATR (Average True Range)\n","    * Other asset price data: On balance volume\n","<br>\n","<br>\n","1. [Reward function](#reward): Think about the outcome you are looking for from the RL model and select the reward function accordingly. You can use the existing reward functions or make your own.\n","<br>\n","1. [Experience replay sampling](#replay): Change the uniform randomly sampling approach to recency sampling. In the recency sampling approach, select N most recent samples from the memory buffer. This can be then restored to uniform random sampling. Recency sampling is expected to perform badly but will act as the baseline to assess the impact of experience replay on the RL agent."]},{"cell_type":"markdown","metadata":{"id":"wkQ1CI7T-Wab"},"source":["<a id='Import_modules'></a> \n","## Import modules"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XBgQX5l9-Wac"},"outputs":[],"source":["import sys\n","sys.path.append(\"..\")\n","\n","# Ignore warnings\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","plt.style.use('seaborn-darkgrid')\n","\n","import pandas as pd\n","import numpy as np\n","from datetime import timedelta\n","from tensorflow.keras import Sequential\n","from tensorflow.keras.layers import Dense\n","from tensorflow.keras.optimizers import SGD\n","import talib\n","import pickle\n","import pyfolio as pf"]},{"cell_type":"markdown","metadata":{"id":"1t2vSb61-Wae"},"source":["<a id='Read_price_data'></a> \n","## Read price data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kyNiOfee-Waf","outputId":"efb83e09-7395-46c0-d818-0e0c43c0747c"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>open</th>\n","      <th>high</th>\n","      <th>low</th>\n","      <th>close</th>\n","      <th>volume</th>\n","    </tr>\n","    <tr>\n","      <th>Time</th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>2020-07-24 23:45:00</th>\n","      <td>0.710</td>\n","      <td>0.710</td>\n","      <td>0.710</td>\n","      <td>0.710</td>\n","      <td>5000.0</td>\n","    </tr>\n","    <tr>\n","      <th>2020-07-24 23:50:00</th>\n","      <td>0.710</td>\n","      <td>0.710</td>\n","      <td>0.710</td>\n","      <td>0.710</td>\n","      <td>5000.0</td>\n","    </tr>\n","    <tr>\n","      <th>2020-07-24 23:55:00</th>\n","      <td>0.710</td>\n","      <td>0.710</td>\n","      <td>0.710</td>\n","      <td>0.710</td>\n","      <td>5000.0</td>\n","    </tr>\n","    <tr>\n","      <th>2020-07-25 00:00:00</th>\n","      <td>0.710</td>\n","      <td>0.710</td>\n","      <td>0.710</td>\n","      <td>0.710</td>\n","      <td>4000.0</td>\n","    </tr>\n","    <tr>\n","      <th>2020-07-27 00:00:00</th>\n","      <td>0.709</td>\n","      <td>0.709</td>\n","      <td>0.709</td>\n","      <td>0.709</td>\n","      <td>324000.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                      open   high    low  close    volume\n","Time                                                     \n","2020-07-24 23:45:00  0.710  0.710  0.710  0.710    5000.0\n","2020-07-24 23:50:00  0.710  0.710  0.710  0.710    5000.0\n","2020-07-24 23:55:00  0.710  0.710  0.710  0.710    5000.0\n","2020-07-25 00:00:00  0.710  0.710  0.710  0.710    4000.0\n","2020-07-27 00:00:00  0.709  0.709  0.709  0.709  324000.0"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["# Read the price data\n","bars5m = pd.read_pickle('../data_modules/fx_pair_data_1.bz2')\n","\n","# Display the last 5 entries of price data\n","bars5m.tail()"]},{"cell_type":"markdown","metadata":{"id":"akgLXI6U-Wah"},"source":["<a id='sanity'></a> \n","## Data sanity check"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CyZancB5-Wai","outputId":"db228db2-b9f6-4f75-bc73-13cb6bca92cd"},"outputs":[{"data":{"text/plain":["'\\nTo maintain consistent data, we consider the dates for which full data is available.\\n# A better option would be to use a reliable data source.\\n'"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["'''\n","This code block will take some time to execute and can be uncommented and tried on your local system.\n","This block might not run on the Quantra platform due to limited resources.\n","'''\n","\n","# # Adding check for missing data\n","# bars5m.groupby('Time').count()\n","# # Groupby data points available on each day\n","# df = bars5m.groupby(bars5m.index.date).count()\n","# # View dates where the data is missing. Since we are dealing with 5 min data, there will be 288 data points in 24 hours.\n","# df[df['open']!=288].head(10)\n","\n","'''\n","To maintain consistent data, we consider the dates for which full data is available.\n","# A better option would be to use a reliable data source.\n","'''\n","# # bars5m=bars5m.drop(df[(df['open']==288)].index)\n","# to_delete = list(pd.to_datetime(df[(df['open']!=288)].index).strftime('%Y-%m-%d'))\n","# bars5m = bars5m[~(bars5m.index.strftime('%Y-%m-%d').isin(to_delete))]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8Z8UE9Yz-Waj","outputId":"f11705a4-f8c3-420d-cdee-66ab6ce075f5"},"outputs":[{"data":{"text/plain":["open      0\n","high      0\n","low       0\n","close     0\n","volume    0\n","dtype: int64"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["# Check for invalid/NaN data\n","bars5m.isnull().sum()"]},{"cell_type":"markdown","metadata":{"id":"Tf-TIS9a-Waj"},"source":["This data has no NaN values."]},{"cell_type":"markdown","metadata":{"id":"viP-Xtnv-Wak"},"source":["<a id='input_features'></a> \n","## Game class and input features\n","We define the game class for the RL agent. The `_assemble_state` function can be edited to add any number of features."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dCFfm2We-Wak"},"outputs":[],"source":["class Game(object):\n","\n","    def __init__(self, bars5m, bars1d, bars1h, reward_function, lkbk=20, init_idx=None):\n","        self.bars5m = bars5m\n","        self.lkbk = lkbk\n","        self.trade_len = 0\n","        self.stop_pnl = None\n","        self.bars1d = bars1d\n","        self.bars1h = bars1h\n","        self.is_over = False\n","        self.reward = 0\n","        self.pnl_sum = 0\n","        self.init_idx = init_idx\n","        self.reward_function = reward_function\n","        self.reset()\n","\n","    def _update_position(self, action):\n","        '''This is where we update our position'''\n","        if action == 0:\n","            pass\n","\n","        elif action == 2:\n","            \"\"\"---Enter a long or exit a short position---\"\"\"\n","\n","            # If the current position (buy) is the same as the action (buy), do nothing\n","            if self.position == 1:\n","                pass\n","\n","            # If there is no current position, we update the position to indicate buy\n","            elif self.position == 0:\n","                self.position = 1\n","                self.entry = self.curr_price\n","                self.start_idx = self.curr_idx\n","\n","            # If action is different than current position, we end the game and get rewards & trade duration\n","            elif self.position == -1:\n","                self.is_over = True\n","\n","        elif action == 1:\n","            \"\"\"---Enter a short or exit a long position---\"\"\"\n","            if self.position == -1:\n","                pass\n","\n","            elif self.position == 0:\n","                self.position = -1\n","                self.entry = self.curr_price\n","                self.start_idx = self.curr_idx\n","\n","            elif self.position == 1:\n","                self.is_over = True\n","\n","    def _assemble_state(self):\n","        '''Here we can add secondary features such as indicators and times to our current state.\n","        First, we create candlesticks for different bar sizes of 5mins, 1hr and 1d.\n","        We then add some state variables such as time of day, day of week and position.\n","        Next, several indicators are added and subsequently z-scored.\n","        '''\n","\n","        \"\"\"---Initializing State Variables---\"\"\"\n","        self.state = np.array([])\n","\n","        self._get_last_N_timebars()\n","\n","        \"\"\"\"---Adding Normalised Candlesticks---\"\"\"\n","\n","        def _get_normalised_bars_array(bars):\n","            bars = bars.iloc[-10:, :-1].values.flatten()\n","            \"\"\"Normalizing candlesticks\"\"\"\n","            bars = (bars-np.mean(bars))/np.std(bars)\n","            return bars\n","\n","        self.state = np.append(\n","            self.state, _get_normalised_bars_array(self.last5m))\n","        self.state = np.append(\n","            self.state, _get_normalised_bars_array(self.last1h))\n","        self.state = np.append(\n","            self.state, _get_normalised_bars_array(self.last1d))\n","\n","        \"\"\"---Adding Techincal Indicators---\"\"\"\n","\n","        def _get_technical_indicators(bars):\n","            # Create an array to store the value of indicators\n","            tech_ind = np.array([])\n","\n","            \"\"\"Relative Strength Index\"\"\"\n","            tech_ind = np.append(tech_ind, talib.RSI(\n","                bars['close'], self.lkbk-1)[-1])\n","\n","            \"\"\"Momentum\"\"\"\n","            tech_ind = np.append(tech_ind, talib.MOM(\n","                bars['close'], self.lkbk-1)[-1])\n","\n","            \"\"\"Balance of Power\"\"\"\n","            tech_ind = np.append(tech_ind, talib.BOP(bars['open'],\n","                                                     bars['high'],\n","                                                     bars['low'],\n","                                                     bars['close'])[-1])\n","\n","            \"\"\"Aroon Oscillator\"\"\"\n","            tech_ind = np.append(tech_ind, talib.AROONOSC(bars['high'],\n","                                                          bars['low'],\n","                                                          self.lkbk-3)[-1])\n","\n","            '''\n","            You can add as many input features you want in this section.\n","            They can be fundamental data as well.\n","            Make sure they are forward-filled and in the same dimension as the existing data.\n","            In addition to the above, let's add the indicators mentioned in the capstone project model solution.\n","        \n","            '''\n","\n","            # Indicators as input features for the capstone project\n","\n","            # --------------- Statistic Functions --------------------------\n","            \"\"\"Beta of the high and low values\"\"\"\n","            tech_ind = np.append(tech_ind, talib.BETA(bars['high'],\n","                                                      bars['low'])[-1])\n","            # --------------------------------------------------------------\n","\n","            # --------------- Overlap Studies ------------------------------\n","            \"\"\"Relative difference two moving averages\"\"\"\n","            ema1 = talib.EMA(bars['close'], self.lkbk-1)[-1]\n","            ema2 = talib.EMA(bars['close'], self.lkbk-8)[-1]\n","            tech_ind = np.append(tech_ind, (ema1-ema2)/ema2)\n","            # --------------------------------------------------------------\n","\n","            # --------------- Volatility Indicators ------------------------\n","\n","            \"\"\"Standard Deviation\"\"\"\n","            tech_ind = np.append(tech_ind, talib.STDDEV(bars['close'])[-1])\n","\n","            \"\"\"Average True Range\"\"\"\n","            tech_ind = np.append(tech_ind, talib.ATR(bars['high'],\n","                                                     bars['low'],\n","                                                     bars['close'])[-1])\n","            # --------------------------------------------------------------\n","\n","            # --------------- Volume Indicators ------------------------\n","            \"\"\"On Balance Volume\"\"\"\n","            tech_ind = np.append(tech_ind, talib.OBV(bars['close'],\n","                                                     bars['volume'])[-1])\n","            # --------------------------------------------------------------\n","\n","            return tech_ind\n","\n","        self.state = np.append(\n","            self.state, _get_technical_indicators(self.last5m))\n","        self.state = np.append(\n","            self.state, _get_technical_indicators(self.last1h))\n","        self.state = np.append(\n","            self.state, _get_technical_indicators(self.last1d))\n","\n","        \"\"\"---Adding Time Signature---\"\"\"\n","        self.curr_time = self.bars5m.index[self.curr_idx]\n","        tm_lst = list(map(float, str(self.curr_time.time()).split(':')[:2]))\n","        self._time_of_day = (tm_lst[0]*60 + tm_lst[1])/(24*60)\n","        self._day_of_week = self.curr_time.weekday()/6\n","        self.state = np.append(self.state, self._time_of_day)\n","        self.state = np.append(self.state, self._day_of_week)\n","\n","        \"\"\"---Adding Position---\"\"\"\n","        self.state = np.append(self.state, self.position)\n","\n","    def _get_last_N_timebars(self):\n","        '''This function gets the timebars for the 5m, 1hr and 1d resolution based\n","        on the lookback we've specified.\n","        '''\n","        wdw5m = 9\n","        wdw1h = np.ceil(self.lkbk*15/24.)\n","        wdw1d = np.ceil(self.lkbk*15)\n","\n","        \"\"\"---Getting candlesticks before current time---\"\"\"\n","        self.last5m = self.bars5m[self.curr_time -\n","                                  timedelta(wdw5m):self.curr_time].iloc[-self.lkbk:]\n","        self.last1h = self.bars1h[self.curr_time -\n","                                  timedelta(wdw1h):self.curr_time].iloc[-self.lkbk:]\n","        self.last1d = self.bars1d[self.curr_time -\n","                                  timedelta(wdw1d):self.curr_time].iloc[-self.lkbk:]\n","\n","    def _get_reward(self):\n","        \"\"\"Here we calculate the reward when the game is finished.\n","        Reward function design is very difficult and can significantly\n","        impact the performance of our algo.\n","        In this case, we use a simple pnl reward but it is conceivable to use\n","        other metrics such as Sharpe ratio, average return, etc.\n","        \"\"\"\n","        if self.is_over:\n","            self.reward = self.reward_function(\n","                self.entry, self.curr_price, self.position)\n","\n","    def get_state(self):\n","        \"\"\"This function returns the state of the system.\n","        Returns:\n","            self.state: the state including indicators, position and times.\n","        \"\"\"\n","        # Assemble new state\n","        self._assemble_state()\n","        return np.array([self.state])\n","\n","    def act(self, action):\n","        \"\"\"This function updates the state based on an action\n","        that was calculated by the NN.\n","        This is the point where the game interacts with the trading\n","        algo.\n","        \"\"\"\n","\n","        self.curr_time = self.bars5m.index[self.curr_idx]\n","        self.curr_price = self.bars5m['close'][self.curr_idx]\n","\n","        self._update_position(action)\n","\n","        # Unrealized or realized pnl. This is different from pnl in reward method which is only realized pnl.\n","        self.pnl = (-self.entry + self.curr_price)*self.position/self.entry\n","\n","        self._get_reward()\n","        if self.is_over:\n","            self.trade_len = self.curr_idx - self.start_idx\n","\n","        return self.reward, self.is_over\n","\n","    def reset(self):\n","        \"\"\"Resetting the system for each new trading game.\n","        Here, we also resample the bars for 1h and 1d.\n","        Ideally, we should do this on every update but this will take very long.\n","        \"\"\"\n","        self.pnl = 0\n","        self.entry = 0\n","        self._time_of_day = 0\n","        self._day_of_week = 0\n","        self.curr_idx = self.init_idx\n","        self.t_in_secs = (\n","            self.bars5m.index[-1]-self.bars5m.index[0]).total_seconds()\n","        self.start_idx = self.curr_idx\n","        self.curr_time = self.bars5m.index[self.curr_idx]\n","        self._get_last_N_timebars()\n","        self.position = 0\n","        self.act(0)\n","        self.state = []\n","        self._assemble_state()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"h-tLJQft-Wao"},"outputs":[],"source":["'''\n","Defining the method to create the neural networks\n","'''\n","\n","def init_net(env, rl_config):\n","    \"\"\"\n","    This initialises the RL run by\n","    creating two new predictive neural network\n","    Args:\n","        env:\n","    Returns:\n","        modelQ: the neural network\n","        modelR: the neural network\n","\n","    \"\"\"\n","    hidden_size = len(env.state)*rl_config['HIDDEN_MULT']\n","    modelQ = Sequential()\n","    modelQ.add(Dense(len(env.state), input_shape=(\n","        len(env.state),), activation=rl_config['ACTIVATION_FUN']))\n","    modelQ.add(Dense(hidden_size, activation=rl_config['ACTIVATION_FUN']))\n","    modelQ.add(Dense(rl_config['NUM_ACTIONS'], activation='softmax'))\n","    modelQ.compile(SGD(lr=rl_config['LEARNING_RATE']),\n","                   loss=rl_config['LOSS_FUNCTION'])\n","\n","    modelR = Sequential()\n","    modelR.add(Dense(len(env.state), input_shape=(\n","        len(env.state),), activation=rl_config['ACTIVATION_FUN']))\n","    modelR.add(Dense(hidden_size, activation=rl_config['ACTIVATION_FUN']))\n","    modelR.add(Dense(rl_config['NUM_ACTIONS'], activation='softmax'))\n","    modelR.compile(SGD(lr=rl_config['LEARNING_RATE']),\n","                   loss=rl_config['LOSS_FUNCTION'])\n","\n","    return modelQ, modelR"]},{"cell_type":"markdown","metadata":{"id":"AbKSsATa-Waq"},"source":["<a id='reward'></a> \n","## Reward function\n","\n","Define the various reward functions here. You can define custom reward functions here. The particular reward function to use for the RL agent will be specified in the `rl_config` dictionary which is used as the configuration."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pFwjxz79-War"},"outputs":[],"source":["def get_pnl(entry, curr, pos):\n","    # Transaction cost and commissions\n","    tc = 0.001\n","    return (curr*(1-tc) - entry*(1+tc))/entry*(1+tc)*pos\n","\n","\n","def reward_pos_log_pnl(entry, curr, pos):\n","    \"\"\"positive log categorical\"\"\"\n","    pnl = get_pnl(entry, curr, pos)\n","    if pnl >= 0:\n","        return np.ceil(np.log(pnl*100+1))\n","    else:\n","        return 0\n","\n","\n","def reward_pure_pnl(entry, curr, pos):\n","    '''pure pnl'''\n","    return get_pnl(entry, curr, pos)\n","\n","\n","def reward_positive_pnl(entry, curr, pos):\n","    '''positive pnl, zero otherwise'''\n","    pnl = get_pnl(entry, curr, pos)\n","    if pnl >= 0:\n","        return pnl\n","    else:\n","        return 0\n","\n","\n","def reward_categorical_pnl(entry, curr, pos):\n","    '''Sign of pnl'''\n","    return np.sign(get_pnl(entry, curr, pos))\n","\n","\n","def reward_positive_categorical_pnl(entry, curr, pos):\n","    '''1 for win, 0 for loss'''\n","    pnl = get_pnl(entry, curr, pos)\n","    if pnl >= 0:\n","        return 1\n","    else:\n","        return 0\n","\n","\n","def reward_exponential_pnl(entry, curr, pos):\n","    '''exp pure pnl'''\n","    return np.exp(get_pnl(entry, curr, pos))"]},{"cell_type":"markdown","metadata":{"id":"qS8A5IEF-War"},"source":["<a id='replay'></a> \n","## Experience replay\n","\n","We define the experience replay class here. The `process` function is used to implement the experience replay.\n","The random sampling as well as recency sampling is provided in code and you may use these as you want."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9UQwj-eY-Was"},"outputs":[],"source":["class ExperienceReplay(object):\n","    '''This class calculates the Q-Table.\n","    It gathers memory from previous experience and \n","    creates a Q-Table with states and rewards for each\n","    action using the NN. At the end of the game the reward\n","    is calculated from the reward function. \n","    The weights in the NN are constantly updated with new\n","    batch of experience. \n","    This is the heart of the RL algorithm.\n","    Args:\n","        state_tp1: the state at time t+1\n","        state_t: the state at time t\n","        action_t: int {0..2} hold, sell, buy taken at state_t \n","        Q_sa: float, the reward for state_tp1\n","        reward_t: the reward for state_t\n","        self.memory: list of state_t, action_t and reward_t at time t as well as state_tp1\n","        targets: array(float) Nx2, weight of each action\n","        inputs: an array with scrambled states at different times\n","        targets: Nx3 array of weights for each action for scrambled input states\n","    '''\n","\n","    def __init__(self, max_memory, discount):\n","        self.max_memory = max_memory\n","        self.memory = list()\n","        self.discount = discount\n","\n","    def remember(self, states, game_over):\n","        '''Add states to time t and t+1 as well as  to memory'''\n","        self.memory.append([states, game_over])\n","        if len(self.memory) > self.max_memory:\n","            del self.memory[0]\n","\n","    def process(self, modelQ, modelR, batch_size=10):\n","        len_memory = len(self.memory)\n","        num_actions = modelQ.output_shape[-1]\n","        env_dim = self.memory[0][0][0].shape[1]\n","\n","        \"\"\"---Initialise input and target arrays---\"\"\"\n","        inputs = np.zeros((min(len_memory, batch_size), env_dim))\n","        targets = np.zeros((inputs.shape[0], num_actions))\n","\n","        # Option 1\n","        \"\"\"\n","        Random Sampling for loop:\n","        Step randomly through different places in the memory\n","        and scramble them into a new input array (inputs) with the\n","        length of the pre-defined batch size\n","        \n","        for i, idx in enumerate(np.random.randint(0, len_memory, size=inputs.shape[0]))\n","        \n","        \"\"\"\n","\n","        # Option 2\n","\n","        \"\"\"\n","        Recency Sampling for loop:\n","        Select sequentially from the most recent memories. The number of memories fetched is\n","        determined by the batch size.\n","        \n","        for i, idx in enumerate(np.arange(-inputs.shape[0],0))\n","        \n","        \"\"\"\n","\n","        # Implementing the recency sampling loop\n","        for i, idx in enumerate(np.arange(-inputs.shape[0], 0)):\n","            \"\"\"Obtain the parameters for Bellman from memory,\n","            S.A.R.S: state, action, reward, new state.\"\"\"\n","            state_t, action_t, reward_t, state_tp1 = self.memory[idx][0]\n","            game_over = self.memory[idx][1]\n","            inputs[i] = state_t\n","\n","            \"\"\"---Calculate the targets for the state at time t---\"\"\"\n","            targets[i] = modelR.predict(state_t)[0]\n","\n","            \"\"\"---Calculate the reward at time t+1 for action at time t---\"\"\"\n","            Q_sa = np.max(modelQ.predict(state_tp1)[0])\n","\n","            if game_over:\n","                \"\"\"---When game is over we have a definite reward---\"\"\"\n","                targets[i, action_t] = reward_t\n","            else:\n","                \"\"\"\n","                ---Update the part of the target for which action_t occurred to new value---\n","                Q_new(s,a) = reward_t + gamma * max_a' Q(s', a')\n","                \"\"\"\n","                targets[i, action_t] = reward_t + self.discount * Q_sa\n","\n","        return inputs, targets"]},{"cell_type":"markdown","metadata":{"id":"ttHEuzeb-Was"},"source":["<a id='backtesting'></a> \n","## Backtesting function\n","\n","We define the `run` function. It is used by the RL agent to train on the historical data."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nf23H_zS-Was"},"outputs":[],"source":["def run(bars5m, rl_config):\n","    \"\"\"\n","    Function to run the RL model on the passed price data\n","    \"\"\"\n","\n","    pnls = []\n","    trade_logs = pd.DataFrame()\n","    episode = 0\n","\n","    ohlcv_dict = {\n","        'open': 'first',\n","        'high': 'max',\n","        'low': 'min',\n","        'close': 'last',\n","        'volume': 'sum'\n","    }\n","\n","    bars1h = bars5m.resample(\n","        '1H', label='right', closed='right').agg(ohlcv_dict).dropna()\n","    bars1d = bars1h.resample(\n","        '1D', label='right', closed='right').agg(ohlcv_dict).dropna()\n","\n","    \"\"\"---Initialise a NN and a set up initial game parameters---\"\"\"\n","    env = Game(bars5m, bars1d, bars1h, rl_config['RF'],\n","               lkbk=rl_config['LKBK'], init_idx=rl_config['START_IDX'])\n","    q_network, r_network = init_net(env, rl_config)\n","    exp_replay = ExperienceReplay(\n","        max_memory=rl_config['MAX_MEM'], discount=rl_config['DISCOUNT_RATE'])\n","    \"\"\"---Preloading the model weights---\"\"\"\n","    if rl_config['PRELOAD']:\n","        q_network.load_weights(rl_config['WEIGHTS_FILE'])\n","        r_network.load_weights(rl_config['WEIGHTS_FILE'])\n","        exp_replay.memory = pickle.load(open(rl_config['REPLAY_FILE'], 'rb'))\n","\n","    r_network.set_weights(q_network.get_weights())\n","\n","    \"\"\"---Loop that steps through one trade (game) at a time---\"\"\"\n","    while True:\n","        \"\"\"---Stop the algo when end is near to avoid exception---\"\"\"\n","        if env.curr_idx >= len(bars5m)-1:\n","            break\n","\n","        episode += 1\n","\n","        \"\"\"---Initialise a new game---\"\"\"\n","        env = Game(bars5m, bars1d, bars1h, rl_config['RF'],\n","                   lkbk=rl_config['LKBK'], init_idx=env.curr_idx)\n","        state_tp1 = env.get_state()\n","\n","        \"\"\"---Calculate epsilon for exploration vs exploitation random action generator---\"\"\"\n","        epsilon = rl_config['EPSILON']**(np.log10(episode)) + \\\n","            rl_config['EPS_MIN']\n","\n","        game_over = False\n","        cnt = 0\n","\n","        \"\"\"---Walkthrough time steps starting from the end of the last game---\"\"\"\n","        while not game_over:\n","\n","            if env.curr_idx >= len(bars5m)-1:\n","                break\n","\n","            cnt += 1\n","            state_t = state_tp1\n","\n","            \"\"\"---Generate a random action or through q_network---\"\"\"\n","            if np.random.rand() <= epsilon:\n","                action = np.random.randint(0, 3, size=1)[0]\n","\n","            else:\n","                q = q_network.predict(state_t)\n","                action = np.argmax(q[0])\n","\n","            \"\"\"---Updating the Game---\"\"\"\n","            reward, game_over = env.act(action)\n","\n","            \"\"\"---Updating trade/position logs---\"\"\"\n","            tl = [[env.curr_time, env.position, episode]]\n","            if game_over:\n","                tl = [[env.curr_time, 0, episode]]\n","            trade_logs = trade_logs.append(tl)\n","\n","            \"\"\"---Move to next time step---\"\"\"\n","            env.curr_idx += 1\n","            state_tp1 = env.get_state()\n","\n","            \"\"\"---Adding state to memory---\"\"\"\n","            exp_replay.remember(\n","                [state_t, action, reward, state_tp1], game_over)\n","\n","            \"\"\"---Creating a new Q-Table---\"\"\"\n","            inputs, targets = exp_replay.process(\n","                q_network, r_network, batch_size=rl_config['BATCH_SIZE'])\n","            env.pnl_sum = sum(pnls)\n","\n","            \"\"\"---Update the NN model with a new Q-Table\"\"\"\n","            q_network.train_on_batch(inputs, targets)\n","\n","            if game_over and rl_config['UPDATE_QR']:\n","                r_network.set_weights(q_network.get_weights())\n","\n","        pnls.append(env.pnl)\n","\n","        print(\"Trade {:03d} | pos {} | len {} | approx cum ret {:,.2f}% | trade ret {:,.2f}% | eps {:,.4f} | {} | {}\".format(\n","            episode, env.position, env.trade_len, sum(pnls)*100, env.pnl*100, epsilon, env.curr_time, env.curr_idx))\n","\n","        if not episode % 10:\n","            print('----saving weights, trade logs and replay buffer-----')\n","            r_network.save_weights(rl_config['WEIGHTS_FILE'], overwrite=True)\n","            trade_logs.to_pickle(rl_config['TRADE_FILE'])\n","            pickle.dump(exp_replay.memory, open(\n","                rl_config['REPLAY_FILE'], 'wb'))\n","\n","        if not episode % 15 and rl_config['TEST_MODE']:\n","            print('\\n**********************************************\\nTest mode is on due to resource constraints and therefore stopped after 15 trades. \\nYou can trade on full dataset on your local computer and set TEST_MODE flag to False in rl_config dictionary. \\nThe full code file, quantra_reinforemcent_learning module and data file is available in last unit of the course.\\n**********************************************\\n')\n","            break\n","\n","    print('----saving weights, trade logs and replay buffer-----')\n","    r_network.save_weights(rl_config['WEIGHTS_FILE'], overwrite=True)\n","    trade_logs.to_pickle(rl_config['TRADE_FILE'])\n","    pickle.dump(exp_replay.memory, open(rl_config['REPLAY_FILE'], 'wb'))\n","\n","    print('***FINISHED***')"]},{"cell_type":"markdown","metadata":{"id":"rvk7uRMd-Wat"},"source":["<a id='run'></a> \n","# RL configuration and running the backtest"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lbDQriqM-Wat"},"outputs":[],"source":["\"\"\"\n","For running the RL model on the price data, you need to \n","set the configuration parameters.\n","These configuration parameters are hyperparameters for the \n","RL model and the ANN used in it.\n","\"\"\"\n","rl_config = {\n","\n","    'LEARNING_RATE': 0.00001,\n","    'LOSS_FUNCTION': 'mse',\n","    'ACTIVATION_FUN': 'relu',\n","    'NUM_ACTIONS': 3,\n","    'HIDDEN_MULT': 2,\n","    'DISCOUNT_RATE': 0.9,\n","    'LKBK': 30,\n","    'BATCH_SIZE': 30,\n","    'MAX_MEM': 600,  # You can change the maxmimum memory buffer here\n","    'EPSILON': 0.0001,\n","    'EPS_MIN': 0.001,\n","    'START_IDX': 5000,\n","    'WEIGHTS_FILE': 'indicator_model_fx_pair_0.h5',\n","    'TRADE_FILE': 'trade_logs_fx_pair_0.bz2',\n","    'REPLAY_FILE': 'memory_fx_pair_0.bz2',\n","    'RF': reward_exponential_pnl,  # You can change the reward function here\n","    'TEST_MODE': True,\n","    'PRELOAD': False,\n","    'UPDATE_QR': True\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rJsBsxO3-Wau","outputId":"89f82ed0-28fc-4f31-f6fb-db2af0ec82c2"},"outputs":[{"name":"stdout","output_type":"stream","text":["Trade 001 | pos 1 | len 2 | approx cum ret -0.23% | trade ret -0.23% | eps 1.0010 | 2008-01-22 08:45:00 | 5003\n","Trade 002 | pos 1 | len 73 | approx cum ret 1.17% | trade ret 1.41% | eps 0.0635 | 2008-01-22 15:15:00 | 5081\n","Trade 003 | pos -1 | len 356 | approx cum ret 1.06% | trade ret -0.11% | eps 0.0133 | 2008-01-24 09:30:00 | 5588\n","Trade 004 | pos 1 | len 124 | approx cum ret 1.86% | trade ret 0.80% | eps 0.0049 | 2008-01-25 02:45:00 | 5795\n","Trade 005 | pos 1 | len 140 | approx cum ret 1.63% | trade ret -0.22% | eps 0.0026 | 2008-01-30 11:15:00 | 7050\n","Trade 006 | pos -1 | len 543 | approx cum ret 0.29% | trade ret -1.35% | eps 0.0018 | 2008-02-03 04:00:00 | 7828\n","Trade 007 | pos -1 | len 449 | approx cum ret 0.06% | trade ret -0.22% | eps 0.0014 | 2008-02-10 14:00:00 | 9677\n","Trade 008 | pos 1 | len 468 | approx cum ret -0.27% | trade ret -0.33% | eps 0.0012 | 2008-02-14 02:40:00 | 10693\n","Trade 009 | pos -1 | len 4156 | approx cum ret -2.92% | trade ret -2.65% | eps 0.0012 | 2008-03-04 07:40:00 | 15364\n","Trade 010 | pos 1 | len 1973 | approx cum ret -6.75% | trade ret -3.84% | eps 0.0011 | 2008-03-21 06:50:00 | 19676\n","----saving weights, trade logs and replay buffer-----\n","Trade 011 | pos -1 | len 2685 | approx cum ret -7.08% | trade ret -0.33% | eps 0.0011 | 2008-04-04 07:10:00 | 23138\n","Trade 012 | pos 1 | len 635 | approx cum ret -7.73% | trade ret -0.64% | eps 0.0010 | 2008-04-13 22:10:00 | 25336\n","Trade 013 | pos -1 | len 2674 | approx cum ret -7.30% | trade ret 0.43% | eps 0.0010 | 2008-04-28 03:20:00 | 28856\n","Trade 014 | pos -1 | len 541 | approx cum ret -6.14% | trade ret 1.17% | eps 0.0010 | 2008-05-14 08:25:00 | 32951\n","Trade 015 | pos 1 | len 9109 | approx cum ret -4.10% | trade ret 2.03% | eps 0.0010 | 2008-06-22 02:15:00 | 42387\n","\n","**********************************************\n","Test mode is on due to resource constraints and therefore stopped after 15 trades. \n","You can trade on full dataset on your local computer and set TEST_MODE flag to False in rl_config dictionary. \n","The full code file, quantra_reinforemcent_learning module and data file is available in last unit of the course.\n","**********************************************\n","\n","----saving weights, trade logs and replay buffer-----\n","***FINISHED***\n"]}],"source":["\"\"\"\n","Run the RL model on the price data\n","Note: To run in a local machine, please change the `TEST_MODE` to \n","`False` in `rl_config`\n","\"\"\"\n","run(bars5m, rl_config)"]},{"cell_type":"markdown","metadata":{"id":"KXhv2vb5-Wau"},"source":["In the next notebook, we see a model solution of combining a few trained RL agents. The model strategy will be then analysed by the pyfolio package. "]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.5"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}